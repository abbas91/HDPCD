Create a folder for storing imported files on hdfs
	hadoop fs -mkdir -p /user/practical_hive/


1)Create Database :->

CREATE DATABASE IF NOT EXISTS practical_hive;

USE cards;

2) CREATE TABLE :->
CREATE EXTERNAL TABLE practical_hive.customers (fname STRING,lname STRING,address STRUCT <HOUSENO:STRING, STREET:STRING, CITY:STRING, ZIPCODE:INT,STATE:STRING, COUNTRY:STRING>,active BOOLEAN,created DATE)COMMENT "customer master record table"LOCATION '/user/practical_hive/customers/';

states.txt
californiaohionorth dakotanew yorkcoloradonew jersey

hadoop fs -put states.txt /user/practical_hive/states

morestates.txt
new mexicohawaiioregonsouth dakota

hadoop fs -put morestates.txt /user/practical_hive/states

CREATE TABLE states_internal(state string) LOCATION '/user/practical_hive/states';
CREATE EXTERNAL TABLE states_external(state string) LOCATION '/user/practical_hive/states';

3) TBLPROPERTIES :->

states3.txt
STATE_NAME----------californiaohionorth dakotanew yorkcoloradonew jersey

hadoop fs -mkdir -p /user/practical_hive/states3/
hadoop fs -put states3.txt /user/practical_hive/states3/

CREATE EXTERNAL TABLE states3(state string) LOCATION '/user/practical_hive/states3â€™;

3) Partitioning : ->
CREATE EXTERNAL TABLE practical_hive.transactions (transdate DATE,transid INT,custid INT,fname STRING,lname STRING,item STRING,qty INT,price FLOAT)PARTITIONED BY (store STRING);

INSERT INTO practical_hive.transactions PARTITION (store="new york") values ("01/25/2016",101,"A109","MATTHEW","SMITH","SHOES",1,112.9);

4) Alter Table - Add Partition:

hadoop fs -mkdir /user/practical_hive/ids
hadoop fs -mkdir /user/practical_hive/ids/2016-05-31
hadoop fs -mkdir /user/practical_hive/ids/2016-05-30

vi 2016-05-31.txt
11
12
13
14
15
16
hadoop fs -put 2016-05-31.txt /user/practical_hive/ids/2016-05-31
vi 2016-05-30.txt
1
2
3
4
5
6
hadoop fs -put 2016-05-30.txt /user/practical_hive/ids/2016-05-30

CREATE EXTERNAL TABLE ids (a INT) PARTITIONED BY(datestamp STRING) LOCATION '/user/practical_hive/ids';

ALTER TABLE ids ADD PARTITION(datestamp='2016-05-31') LOCATION '/user/practical_hive/ids/2016-05-31';

ALTER TABLE ids ADD PARTITION(datestamp='2016-05-30') LOCATION '/user/practical_hive/ids/2016-05-30';

USE MSCK REPAIR TABLE to add partitions for managed/internal tables:
CREATE TABLE ids_internal (a INT) PARTITIONED BY (datestamp STRING);
INSERT INTO ids_internal PARTITION (datestamp='2016-05-30') values (1);
INSERT INTO ids_internal PARTITION (datestamp='2016-05-31') values (11);
hadoop fs -mkdir /apps/hive/warehouse/ids_internal/datestamp=2016-05-21
vi 2016-05-21.txt
21
22
23
24
hadoop fs -put 2016-05-21.txt /apps/hive/warehouse/ids_internal/datestamp=2016-05-21
MSCK REPAIR TABLE ids_internal;
SHOW PARTITIONs ids_internal;
OK
datestamp=2016-05-21
datestamp=2016-05-30
datestamp=2016-05-31
Time taken: 0.967 seconds, Fetched: 3 row(s)

5) ALTER TABLE - RENAME PARTION :-> (Only External Tables)

ALTER TABLE ids PARTITION (datestamp='2016-05-31') RENAME to PARTITION(datestamp='31-05-2016');

6) ALTER TABLE - Modify Columns :->

ALTER TABLE RETAIL.TRANSACTIONS ADD COLUMNS (loyalty_card boolean);

7) DROP TABLE :->
   DROP TABLE <TABLE_NAME>; -> Will move the table to trash
   DROP TABLE <TABLE_NAME> PURGE; -> Will Delete from  Trash Too

   ALTER TABLE transactions DROP PARTITION (store='oakdrive');

   ALTER TABLE transactions ENABLE NO_DROP;
   ALTER TABLE transactions ENABLE OFFLINE;
   ALTER TABLE <TABLE_NAME> PARTITION <PARTITION_SPEC> ENABLE OFFLINE;

8) CREATE TABLE retail.transactions _top100 STORED AS ORCFILEASSELECT * FROM retail.transactions WHERE custid<101;

CREATE TABLE transactions_test LIKE transactions; -> Copy only schema no data

9) DML :->

scp -P 2222 Person001.csv root@192.168.28.130:/root/data
hadoop fs -put Person001.csv /user/hive_demo
CREATE DATABASE census;
USE census;
CREATE TABLE person (persid int,lastname string,firstname string)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

LOAD DATA INPATH '/user/hive_demo/Person001.csv' OVERWRITE INTO TABLE person;

CREATE TABLE personhub (persid int);

INSERT OVERWRITE TABLE personhub SELECT DISTINCT persid FROM person;

Writing Data to HDFS :->

hadoop fs -mkdir /user/hive_demo/hive_query_output
INSERT OVERWRITE DIRECTORY '/user/hive_demo/hive_query_output'
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
SELECT persid, firstname, lastname FROM person;

Insert and Update :->
USE census;CREATE TABLE census.person20 (persid int,lastname string,firstname string)CLUSTERED BY (persid) INTO 1 BUCKETSSTORED AS orcTBLPROPERTIES('transactional' = 'true');INSERT INTO TABLE person20 VALUES (0,'A','B'),(2,'X','Y');
UPDATEcensus.person20SET lastname = 'SS'WHEREpersid = 0;

DELETE :->
CREATE TABLE census.person30 (persid int,lastname string,firstname string)CLUSTERED BY (persid) INTO 1 BUCKETSSTORED AS orcTBLPROPERTIES('transactional' = 'true');INSERT INTO TABLE census.person30VALUES (0,'A','B'),(2,'X','Y');
DELETE FROM census.person30WHERE persid = 0;

Create Table Using LIKE :->
CREATE TABLE person40 LIKE person;
INSERT INTO TABLE person40 VALUES (0,'Bob','Burger'),(1,'Charlie','Clown');





   
    