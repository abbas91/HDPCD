
Create hfs directory for PIG :

hadoop fs -mkdir /user/pig_demo

Create pig_word_count_demo.txt with some text. Transfer it to /user/pig_demo

hadoop fs -put pig_word_count_demo.txt /user/pig_demo

1) pig_demo.pig:

load_data = load '/user/pig_demo/pig_word_count_demo.txt’;
dump load_data;

2) word_count.pig

lines = LOAD '/user/pig_demo/pig_word_count_demo.txt’ using PigStorage() AS (line:chararray);
words = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) AS word;
grouped = GROUP words by word;
wordCount = FOREACH grouped GENERATE group, COUNT(words);
DUMP wordCount;

3) Load data without a schema :->
  
   A = LOAD ‘/user/sqoop/all_tables/departments’ USING PigStorage(‘,’);

4) Load data with a schema :->
  
   A = LOAD ‘/user/sqoop/all_tables/departments’ USING PigStorage(‘,’) AS (department_id:int, department_name:chararray);

5) Load data from Hive :->
   
   Important Launch pig with Parameter  -useHcatalog :-> pig -useHCatalog
   customerDetails = LOAD 'xademo.customer_details' USING org.apache.hive.hcatalog.pig.HCatLoader();
   DUMP customerDetails;

6) Transform to match Hive schema :
   
   


