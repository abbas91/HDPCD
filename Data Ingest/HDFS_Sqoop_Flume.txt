1) To SCP into hortonworks sandbox use the below command
	scp file-name root@192.168.28.129:/root/data
    
2) To create a folder under /user directory :
	sudo -u hdfs hadoop fs -mkdir /user/root
    
3) To change the owner of the new directory to root :
	sudo -u hdfs hadoop fs -chown root:hdfs /user/root

4) To copy a file from local file system to HDFS:
	* First Create A File using : touch testing1.txt (under root or any directory)
	* Edit it using : vi testing1.txt
	* Use Command : hadoop fs -copyFromLocal /root/testing1.txt /user/root
	* Use Command : hadoop fs -put /root/testing1.txt /user/root ( alternative )
	* To Verify : hadoop fs -ls /user/root ( list all files under /user/root)
	* To display contents of a file : hadoop fs -cat /user/root/testing1.txt
	* To connect to a different IP address use : hadoop fs -ls hdfs://192.168.28.129/user/root (hdfs://{ip-address}/{directory})

5) mkdir
	* To create a user space in HDFS use mkdir
	* Use -p to recursively create directories : hadoop fs -mkdir /user/root/dir1/dir2/dir3
	* To Verify use : hadoop fs -ls -R /user/root
	* Use Patterns in mkdir : hadoop fs -mkdir /user/root/t{1,2} (t{1..10})
	* Use Ip address : hadoop fs -mkdir hdfs://192.168.28.129/user/root/a1 hdfs://192.168.28.129/user/root/a2

6) sqoop create directory for practice :
	* hadoop fs -mkdir /user/sqoop
	* sudo -u hdfs hadoop fs -chown root:hdfs /user/sqoop
	* sudo -u hdfs hadoop fs -chmod 777 /user/sqoop
	* hadoop fs -mkdir /user/sqoop/practice1
	* sudo -u hdfs hadoop fs -chown root:hdfs /user/sqoop/practice1
	* sudo -u hdfs hadoop fs -chmod 777 /user/sqoop/practice1 
	
7) sqoop list-databases command :

	sqoop list-databases \
	—-connect “jdbc:mysql://192.168.28.129:3306” \
	—-username retail_dba
	—-password hadoop

8) sqoop list-tables command :
	
	sqoop list-tables \
	—-connect “jdbc:mysql://192.168.28.129:3306/retail_db” \
	—-username retail_dba \
	—-password hadoop

9) sqoop eval command :

	sqoop eval \
	—-connect “jdbc:mysql://sandbox.hortonworks.com:3306/retail_db” \
	—-username retail_dba \
	—-password hadoop \
	—-query “SELECT * FROM departments”

10) sqoop import all tables :
	
	* Create directory for storing all the tables -> hadoop fs -mkdir /user/sqoop/practice1/all_tables
	* sudo -u hdfs hadoop fs -chown root:hdfs /user/sqoop/practice1/all_tables
	* sudo -u hdfs hadoop fs -chmod 777 /user/sqoop/practice1/all_tables
	* sqoop import-all-tables \
	  --connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \
	  --username retail_dba \
	  --password haddop \
	  -m 1 \
	  --warehouse-dir /user/sqoop/practice1/all_tables

11) sqoop import-all-tables into hive :
	* Launch hive -> hive
	* Create Database -> create database sqoop_import
	* sqoop import-all-tables \
	 --connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \
	 --username retail_dba \
	 --password hadoop \
	 --hive-import \
	 --hive-overwrite \
	 --create-hive-table \
	 --hive-database sqoop_import \
	 --compress \
	 --compression-codec org.apache.hadoop.io.compress.SnappyCodec

12) sqoop simple import :
	
	* Create target Directory : hadoop fs -mkdir /user/root/departments
	* Change Owner : sudo -u hdfs hadoop fs -chown root:hdfs /user/root/departments
	* Change Permissions : sudo -u hdfs hadoop fs -chmod 777 /user/root/departments
	* sqoop import \	
	  —-connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \
	  —-username retail_dba \
	  —-password hadoop \
	  —-table departments \
	  —-target-dir /user/root/departments 

13) sqoop boundary-query and Columns:
	* To Remove a file : hadoop fs -rm -R /user/root/departments
	* sqoop import \	
	  —-connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \
	  —-username retail_dba \
	  —-password hadoop \
	  —-table departments \
	  —-target-dir /user/root/departments \
	  -m 2 \
	  --boundary-query "select min(department_id), max(department_id) from departments where department_id <> 8000" \
          --columns department_name

14) sqoop split-by column_name when there is no primary key on a table :
	
	  sqoop import \	
	  —-connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \
	  —-username retail_dba \
	  —-password hadoop \
	  —-table departments_nopk \
	  —-target-dir /user/root/departments_nopk \
	  —split-by department_id

15) sqoop —query argument demo :

sqoop import \	
—-connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \
—-username retail_dba \
—-password hadoop \
—query “select * from orders join order_items on orders.order_id = order_items.order_item_order_id where \$CONDITIONS” \
—-table departments_nopk \
—-target-dir /user/root/orders_join \
—split-by order_id


16) sqoop where argument demo :

sqoop import \	
	  —-connect jdbc:mysql://sandbox.hortonworks.com:3306/retail_db \
	  —-username retail_dba \
	  —-password hadoop \
	  —-table departments \
	  —-target-dir /user/root/departments \
          —append \
	  -m 2 \
	  —where "department_id > 8" \
          --columns department_name
	
	